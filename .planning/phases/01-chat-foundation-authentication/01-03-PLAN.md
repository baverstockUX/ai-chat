---
phase: 01-chat-foundation-authentication
plan: 03
type: execute
wave: 2
depends_on: ["01-01", "01-02"]
files_modified:
  - app/api/chat/route.ts
  - lib/ai/client.ts
  - lib/ai/prompts.ts
  - lib/db/queries.ts
autonomous: true

must_haves:
  truths:
    - "User can send message and receive streaming AI response"
    - "AI response starts appearing within 2 seconds"
    - "Response streams word-by-word, not all at once"
    - "Only authenticated users can access chat API"
  artifacts:
    - path: "app/api/chat/route.ts"
      provides: "Streaming chat API endpoint"
      exports: ["POST"]
      contains: "streamText"
      min_lines: 40
    - path: "lib/ai/client.ts"
      provides: "AI SDK client configuration"
      exports: ["gemini"]
    - path: "lib/ai/prompts.ts"
      provides: "System prompts and AI configuration"
      exports: ["systemPrompt"]
  key_links:
    - from: "app/api/chat/route.ts"
      to: "lib/ai/client.ts"
      via: "import gemini model"
      pattern: "gemini.*flash"
    - from: "app/api/chat/route.ts"
      to: "@ai-sdk/google"
      via: "streamText call"
      pattern: "streamText.*model.*messages"
    - from: "app/api/chat/route.ts"
      to: "app/(auth)/auth.ts"
      via: "auth() call"
      pattern: "await auth\\(\\)"
---

<objective>
Implement streaming AI chat API using Vercel AI SDK with Google Gemini 3 Flash. Handle authentication, message persistence, and proper streaming response format.

Purpose: Enable core chat functionality (CHAT-01) with streaming responses and authentication protection.

Output: Working API endpoint at /api/chat that streams AI responses and saves messages to database.
</objective>

<execution_context>
@/Users/christian.baverstock/.claude/get-shit-done/workflows/execute-plan.md
@/Users/christian.baverstock/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@/Users/christian.baverstock/code/ai-chat/.planning/PROJECT.md
@/Users/christian.baverstock/code/ai-chat/.planning/ROADMAP.md
@/Users/christian.baverstock/code/ai-chat/.planning/phases/01-chat-foundation-authentication/01-CONTEXT.md
@/Users/christian.baverstock/code/ai-chat/.planning/phases/01-chat-foundation-authentication/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Set up Google Gemini AI SDK client</name>
  <files>
lib/ai/client.ts
lib/ai/prompts.ts
.env.local
  </files>
  <action>
Configure Vercel AI SDK for Google Gemini:

**lib/ai/client.ts:**
Create and export Gemini client:
```typescript
import { google } from '@ai-sdk/google';

// Export Gemini Flash 3.0 model
export const gemini = google('gemini-3.0-flash-latest', {
  // Configuration options
});
```

Note: User has GEMINI_KEY in .env already. Rename to GOOGLE_API_KEY (standard AI SDK env var name) or use custom apiKey in config.

**lib/ai/prompts.ts:**
Define system prompt for the AI assistant:
- Professional and helpful tone
- Focused on operations teams and workflow automation
- Clear and concise responses
- Acknowledge when it doesn't know something

Keep prompt simple for Phase 1 - just basic chat assistant behavior. Phase 2 will add orchestration logic.

Example structure:
```typescript
export const systemPrompt = `You are a helpful AI assistant...`;
```

Verify GOOGLE_API_KEY or GEMINI_KEY is available in environment. If using GEMINI_KEY, either rename it or configure AI SDK to use custom key name.
  </action>
  <verify>
1. Check lib/ai/client.ts imports @ai-sdk/google successfully
2. Verify environment variable is accessible (GOOGLE_API_KEY or GEMINI_KEY)
3. Check lib/ai/prompts.ts exports systemPrompt string
  </verify>
  <done>
Gemini AI client configured and ready to use. System prompt defined for Phase 1 chat assistant behavior.
  </done>
</task>

<task type="auto">
  <name>Create streaming chat API route with authentication</name>
  <files>
app/api/chat/route.ts
lib/db/queries.ts (add message CRUD functions)
  </files>
  <action>
Implement streaming chat API following research Pattern 1:

**app/api/chat/route.ts:**
Create POST endpoint that:
1. Checks authentication with `await auth()` - return 401 if not authenticated
2. Extracts `messages` and `conversationId` from request body
3. Validates conversationId belongs to authenticated user
4. Creates new conversation if conversationId is null/undefined
5. Calls `streamText()` from AI SDK with:
   - model: gemini from lib/ai/client.ts
   - messages: user's message history
   - system: systemPrompt from lib/ai/prompts.ts
   - abortSignal: req.signal (for cancellation)
   - onFinish: async callback to save both user message and AI response to database
6. Returns `result.toUIMessageStreamResponse()`

Add `export const maxDuration = 30;` to prevent Vercel timeout (research Pitfall 1).

Use Edge Runtime for better streaming performance:
```typescript
export const runtime = 'edge';
```

**lib/db/queries.ts additions:**
Add functions for message and conversation operations:
- `createConversation(userId: string, title: string)` - create conversation
- `getConversation(id: string, userId: string)` - fetch with user check
- `getConversationMessages(conversationId: string)` - fetch message history
- `createMessage(conversationId: string, role: 'user' | 'assistant', content: string)` - save message
- `generateConversationTitle(firstMessage: string)` - simple title generation from first message (truncate to 50 chars)

Per user decision: conversations auto-name from first message. Generate title in onFinish callback when it's the first message.

Ensure all queries filter by userId for data isolation (AUTH-04 requirement).
  </action>
  <verify>
1. Run `npm run dev` - no TypeScript errors
2. Check app/api/chat/route.ts has auth() check at start
3. Verify maxDuration and runtime exports
4. Check onFinish callback saves messages to database
5. Verify result.toUIMessageStreamResponse() is returned (not plain Response)
  </verify>
  <done>
Streaming chat API endpoint created with authentication, conversation management, message persistence, and proper streaming headers. Edge Runtime configured for performance.
  </done>
</task>

</tasks>

<verification>
1. Start dev server and authenticate
2. Use curl or Postman to test POST /api/chat:
   ```bash
   curl -X POST http://localhost:3000/api/chat \
     -H "Content-Type: application/json" \
     -d '{"messages": [{"role": "user", "content": "Hello"}]}'
   ```
3. Verify response has Content-Type: text/event-stream
4. Check response starts streaming within 2 seconds
5. Verify messages saved to database after stream completes
6. Test without auth - returns 401
</verification>

<success_criteria>
- POST /api/chat accepts messages and returns streaming response
- Response uses proper SSE format (text/event-stream)
- Authentication required - 401 for unauthenticated requests
- Messages saved to database after streaming completes
- New conversations created automatically with auto-generated title
- Conversation ownership validated (users can't access others' conversations)
</success_criteria>

<output>
After completion, create `.planning/phases/01-chat-foundation-authentication/01-03-SUMMARY.md` documenting:
- Google Gemini configuration choices
- API route implementation details (Edge Runtime, streaming headers)
- Message persistence strategy (when and how messages are saved)
- Conversation title generation approach
- Any deviations from research patterns
</output>
